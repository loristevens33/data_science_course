---
title: "Movie Recommendation System"
author: "Lori Stevens"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output:
  word_document:
    toc: TRUE
    toc_depth: 3
    fig_height: 3
---
***
# 1 Introduction

Recommendation systems are designed to understand and predict user preferences based on observed behavior. This is a critical form of machine learning in Retail and e-Commerce industries to personalize interactions with customers. Many consumers interact with recommendation systems daily, receiving suggestions for products or movies of interest on websites and streaming services.

This report details the creation of a movie recommendation system using the MovieLens dataset collected by GroupLens research. The objective is to train a machine learning algorithm to predict moving ratings utilizing a subset of the 10M version of the MovieLens dataset. This is a collection of 10M ratings released in January 2009. The full dataset is available at the following location:

<https://grouplens.org/datasets/movielens/latest/>

Four models are created and compared in this report. The **root mean squared error (RMSE)** is used to evaluate and identify the best performing algorithm. The RMSE is a measure of the algorithm's error in the prediction of user ratings. The goal is to have an RMSE significantly lower than the baseline RMSE to be calculated in the modeling process. All code is referenced directly in this report to allow for easy review of approach, findings, and code.

# 2 Analysis

This section explains the process and techniques used, including data creation, exploration, visualization, modeling approach, and insights.

## 2.1 Data Creation

The first step is to create the edx and validation datasets. The edx dataset is further split into a train and test dataset, to train and test the models. The validation dataset will be used to evaluate the performance of the final algorithm.

```{r Data Creation, echo=TRUE, message=FALSE, warning=FALSE}
# Install packages
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# Load libraries
library(dplyr)
library(tidyverse)
library(tidyr)
library(stringr)
library(ggplot2)
library(caret)
library(lubridate)

# Download the MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>%
  mutate(movieId = as.numeric(levels(movieId))[movieId],
  title = as.character(title),
  genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Create validation dataset (10% of MovieLens dataset)
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Validate userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

# Remove unnecessary data
rm(dl, ratings, movies, test_index, temp, movielens, removed)

# Convert timestamp to YYYY-MM-DD HH:MM:SS format
edx$timestamp <- as.POSIXct(edx$timestamp, origin="1970-01-01")
validation$timestamp <- as.POSIXct(validation$timestamp, origin="1970-01-01")

# Create year column in datasets
edx$year <- format(edx$timestamp,"%Y")
validation$year <- format(validation$timestamp,"%Y")

# Create test dataset (20% of the edx dataset)
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
test_set <- edx[test_index,]

# Create train dataset (80% of the edx dataset)
train_set <- edx[-test_index,]

# Validate userId and movieId in test set are also in train set
test_set <- test_set %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```

## 2.2 Data Exploration and Visualization

The dataset includes seven variables including the User ID, Movie ID, Genre, Title, Rating, Timestamp, and Year. Each row is representative of a movie rating by a single user. The genre assigned can be inclusive of a combination of defined genres. An example of the data is as follows:

```{r Data Sample Records, echo=TRUE, message=FALSE, warning=FALSE}
# Generate sample records of edx dataset
head(edx) %>%
  knitr::kable()
```

A summary of the edx dataset validates that there are no missing values. Ratings range on a scale from 0.5 to 5.0.

```{r Data Summary, echo=TRUE, message=FALSE, warning=FALSE}
# Create summary of edx dataset
summary(edx)
```

The edx dataset is inclusive of over 10K unique movies by approximately 70K users.

```{r Unique Users and Movies, echo=TRUE, message=FALSE, warning=FALSE}
# Calculate total number of unique users and movies in edx dataset
edx %>%
  summarize(unique_users = n_distinct(userId), unique_movies = n_distinct(movieId)) %>%
   knitr::kable()
```

Users have a tendency to rate movies higher overall, with the most frequent rating of 4.0. In addition, users have a tendency to round ratings as half ratings are far less common than full star ratings.

```{r Bar Chart by Rating, echo=TRUE, message=FALSE, warning=FALSE}
# Create bar chart by rating in edx dataset
edx %>%
  group_by(rating) %>%
  summarize(rpercent = n() / nrow(edx) * 100) %>%
  ggplot(aes(rating, rpercent)) +
  geom_col(color = "black",
           fill = "gray50") +
  xlab("Ratings") +
  ylab("% of Ratings") +
  geom_vline(aes(xintercept = 3.5),
    size = 0.5, color = "blue",
    linetype = "dashed") +
  geom_text(aes(x = 2, y = 15, label = "Average = 3.5")) +
  ggtitle("Distribution of Ratings") +
  theme(plot.title = element_text(hjust = 0.5))
```

The median number of ratings per movie has been declining over the years included in the dataset. 2009 was excluded from the chart below as only 5 days of data was available.

```{r Boxplot of Ratings by Year, echo=TRUE, message=FALSE, warning=FALSE}
# Create boxplot of movie ratings by year
edx %>%
  group_by(movieId) %>%
  filter(year != "2009") %>%
  summarize(n = n(), year = as.character(first(year))) %>%
  qplot(year, n, data = ., geom = "boxplot") +
  scale_y_log10() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("Year") +
  ylab("Ratings (log10)") +
  ggtitle("Volume of Ratings by Year")+
  theme(plot.title = element_text(hjust = 0.5))
```

The volume of ratings each movie has received varies significantly, with select movies having a very large or small number of ratings associated. It can be seen in the chart below that there is a particularly large volume of outliers with a single rating.

To ensure that these outliers do not impact the model, **regularization** is used in the models to include a penalty term for small sample sizes. This is outlined in more detail in the **Data Modeling** section.

```{r Histogram of Ratings per Movie, echo=TRUE, message=FALSE, warning=FALSE}
# Create histogram of volume of ratings per movie
edx %>%
  dplyr::count(movieId) %>%
  ggplot(aes(n)) +
  geom_histogram(color = "gray50") +
  scale_x_log10() +
  geom_vline(aes(xintercept = 843),
  size = 0.5, color = "blue",
  linetype = "dashed") +
  geom_text(aes(x = 8000, y = 500, label = "Average = 843")) +
  xlab("Ratings") +
  ylab("Movies") +
  ggtitle("Ratings per Movie")+
  theme(plot.title = element_text(hjust = 0.5))
```

It can be seen in the chart below that many users have rated a large volume of movies. To account for this, a penalty term for users is incorporated into the model as well.

```{r Histogram of Ratings per User, echo=TRUE, message=FALSE, warning=FALSE}
# Create histogram of volume of ratings per user
edx %>%
  dplyr::count(userId) %>%
  ggplot(aes(n)) +
  geom_histogram(color = "gray50") +
  scale_x_log10() +
  geom_vline(aes(xintercept = 129),
  size = 0.5, color = "blue",
  linetype = "dashed") +
  geom_text(aes(x = 500, y = 5000, label = "Average = 129")) +
  xlab("Ratings") +
  ylab("Users") +
  ggtitle("Ratings per User")+
  theme(plot.title = element_text(hjust = 0.5))
```

In addition, users vary significantly on how critical they are with their ratings. Select users have a tendency to give lower or higher than average star ratings. A penalty term for highly critical users is also incorporated into the model to account for this.

```{r Histogram of Mean Ratings by User, echo=TRUE, message=FALSE, warning=FALSE}
# Create histogram of mean ratings by user
edx %>%
  group_by(userId) %>%
  summarize(mean_rating = mean(rating)) %>%
  ggplot(aes(mean_rating)) +
  geom_histogram(color = "gray50") +
  xlab("Mean Rating") +
  ylab("Users") +
  ggtitle("Mean Ratings by User")+
  theme(plot.title = element_text(hjust = 0.5))
```

## 2.3 Data Modeling

The **root mean squared error (RMSE)** is used to evaluate and identify the best performing algorithm. The RMSE is a measure of the algorithm's error in the prediction of user ratings. The function that computes the RMSE is as follows:

$$RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i}(\hat{y}_{u,i}-y_{u,i})^{2}}$$

```{r Create RMSE Function, echo=TRUE, message=FALSE, warning=FALSE}
# Create RMSE function
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))}
```

A successful model will have an RMSE significantly lower than the baseline RMSE. The baseline RMSE is calculated by a basic model that assumes the same rating for all movies and users. The additional models incorporate penalty terms for small sample sizes and highly active or critical users.

### 2.3.1 Baseline Model

The baseline model is simple model that makes the assumption that all differences in movie ratings are explained by random variation. $\mu$ represents the true rating for all movies and users, and $\epsilon$ represents independent errors sampled from the same distribution centered at zero.

$$ Y_{u,i} = \mu + \epsilon_{u,i} $$
In this case, the least squares estimate of $\mu$ is the average rating of all movies across all users. The least squares estimate of $\mu$ is the estimate that minimizes the root mean squared error. This is calculated as the average of all ratings. The average rating is approximately 3.5 and the baseline RMSE is **1.0599**. This will be used for comparison purposes only.

```{r Baseline Model, echo=TRUE, message=FALSE, warning=FALSE}
# Calculate average of train dataset
mu_hat <- mean(train_set$rating)
print(mu_hat)

# Calculate RMSE of baseline model
naive_rmse <- RMSE(test_set$rating, mu_hat)
print(naive_rmse)

# Baseline model RMSE results table
rmse_results <- data_frame(Method = "Baseline Model", RMSE = naive_rmse)
rmse_results %>% knitr::kable()
```

### 2.3.2 Movie Effect Model

To improve the model, a penalty term, $b_{i}$, is incorporated that represents the average rating for each movie, ${i}$. This will take into account the bias, $b$, that popular movies are generally rated higher, and vice versa for unpopular movies.

$$Y_{u,i} = \mu + b_{i} + \epsilon_{u,i}$$

In this case, $b_{i}$ is the average of $Y_{u,i}$ minus the overall mean for each movie $i$. In this model, the penalty term is incorporated for the movie effect which improves the RMSE to **0.9437**.

```{r Movie Effect Model, echo=TRUE, message=FALSE, warning=FALSE}
# Calculate average of train dataset
mu <- mean(train_set$rating)
print(mu)

# Calculate bias of each movie in train dataset
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))

# Predict ratings with movie effect model
predicted_ratings <- mu + test_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  .$b_i

# Calculate RMSE of movie effect model
model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
print(model_1_rmse)

# Add movie effect model RMSE results to table
rmse_results <- bind_rows(rmse_results, data_frame(Method = "Movie Effect Model", RMSE = model_1_rmse))
rmse_results %>% knitr::kable()
```

### 2.3.3 User Effect Model

To further improve the model, the penalty term, $b_{u}$, is incorporated that represents the average rating for each user, ${u}$. This will take into account the bias, $b$, that some users have a tendency to either be highly critical or overly positive in their ratings of movies. This also includes the movie effect from the previous model, $b_{i}$.

$$Y_{u,i} = \mu + b_{i} + b_{u} +  \epsilon_{u,i}$$

In this case, $b_{u}$ is the average of $Y_{u,i}$ minus the overall mean for each user $u$. In this model, the penalty term is incorporated for the movie effect and the user effect which improves the RMSE to **0.8659**.

```{r User Effect Model, echo=TRUE, message=FALSE, warning=FALSE}
# Calculate bias of each user in train dataset
user_avgs <- train_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

# Predict ratings with user effect model
predicted_ratings <- test_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

# Calculate RMSE of user effect model
model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
print(model_2_rmse)

# Add user effect model RMSE results to table
rmse_results <- bind_rows(rmse_results, data_frame(Method = "User Effect Model", RMSE = model_2_rmse))
rmse_results %>% knitr::kable()
```

### 2.3.4 Regularization Model

The final improvement to the model is **regularization**. This will penalize large estimates that come from small sample sizes. This will take into account that some movies have very few ratings and some users only rated a small number of movies. To ensure that these small sample sizes do not influence the prediction, the tuning parameter lambda, $\lambda$, is identified through this process and incorporated into the model. This model also includes the movie effect and user effect from the previous model, $b_{i}$ and $b_{u}$ respectively.

$$\frac{1}{N} + \displaystyle\sum_{u,i} (y_{u,i} - \mu - b_{i} - b_{u})^2 +  \lambda(\displaystyle\sum_{i}b_{i}^2 + \displaystyle\sum_{u}b_{u}^2)$$

In this case, $\lambda$ is the best tuning parameter identified in the cross-validation process. The $\lambda$ selected is **4.75**. In this model, regularization is incorporated into the penalty terms for the movie effect and the user effect which improves the RMSE to **0.8652**.

```{r Lambda Evaluation, echo=TRUE, message=FALSE, warning=FALSE}
# Create sequence of potential lambdas to evaluate
lambdas <- seq(0, 10, 0.25)

# Use cross validation to identify ideal lambda as a tuning parameter
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- train_set %>%
    left_join(b_i, by = 'movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  predicted_ratings <- test_set %>%
    left_join(b_i, by = 'movieId') %>%
    left_join(b_u, by = 'userId') %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))})

# Plot lambas for evaluation
qplot(lambdas, rmses) +
  geom_vline(xintercept=4.75, col = "blue", linetype = "dashed") +
  geom_text(data = data.frame(x=5.5, y=0.8654), mapping = aes(x, y, label="4.75"), color="blue") +
  xlab("Lambda") +
  ylab("RMSE") +
  ggtitle("RMSE by Lambda") +
  theme(plot.title = element_text(hjust = 0.5))

# Save optimal lambda for use in model
lambda <- lambdas[which.min(rmses)]
```

```{r Regularization Model, echo=TRUE, message=FALSE, warning=FALSE}
# Calculate regularized estimate of b_i
b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))

# Calculate regularized estimate of b_u
b_u <- train_set %>%
    left_join(b_i, by = 'movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

# Predict ratings with regularization model
predicted_ratings <- test_set %>%
    left_join(b_i, by = 'movieId') %>%
    left_join(b_u, by = 'userId') %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)

# Calculate RMSE of regularization model
model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
print(model_3_rmse)

# Add regularization model RMSE results to table
rmse_results <- bind_rows(rmse_results, data_frame(Method = "Regularization Model", RMSE = model_3_rmse))
rmse_results %>% knitr::kable()
```

# 3 Results

The final step is to evaluate the final model with the validation dataset. In prior steps, the train and test dataset have been utilized to identify the best model. The validation dataset is then used to evaluate the performance of the final algorithm. The final RMSE is **0.8648**.

```{r Final Evaluation, echo=TRUE, message=FALSE, warning=FALSE}
# Calculate regularized estimate of b_i
b_i <- edx %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))

# Calculate regularized estimate of b_u
b_u <- edx %>%
    left_join(b_i, by = 'movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

# Predict ratings with regularization model
predicted_ratings <- validation %>%
    left_join(b_i, by = 'movieId') %>%
    left_join(b_u, by = 'userId') %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)

# Calculate RMSE of regularization model
final_rmse <- RMSE(predicted_ratings, validation$rating)
print(final_rmse)

# Add final calculation of RMSE results to table
rmse_results <- bind_rows(rmse_results, data_frame(Method = "RMSE w/Validation", RMSE = final_rmse))
rmse_results %>% knitr::kable()
```

# 4 Conclusion

By taking into account movie and user bias as well as regularization to address sample size variation, the machine learning algorithm to predict ratings and recommend movies was improved significantly. The model that included all elements incorporated has a low RMSE of **0.8648**.

```{r Bar Chart by Model, echo=TRUE, message=FALSE, warning=FALSE}
# Create bar chart by model
rmse_results %>%
  mutate(Method = reorder(Method, RMSE)) %>%
  ggplot(aes(Method, RMSE)) +
  geom_bar(stat="identity", show.legend = FALSE) +
  geom_col(color = "black",
           fill = "gray50") +
  coord_flip() +
  xlab("Model") +
  ylab("RMSE") +
  ggtitle("RMSE by Model") +
  theme(plot.title = element_text(hjust = 0.15))
```

The RMSE may be further improved through evaluation and incorporation of other potential effects. Examples could include but are not limited to genre preferences and trends in ratings over time. The possibilities are endless, however, limited in this case to the data available within the MovieLens dataset.